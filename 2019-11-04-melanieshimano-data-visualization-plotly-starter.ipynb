{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization with Plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we reviewed earlier, a great way to convey the results of your data analysis is through clear and eye-catching data visualizations. There are several excellent data visualization Python packages that allow you to customize your displayed data in the best way possible, and in this course, we'll focus on using [Plotly](https://medium.com/plotly/plotly-py-4-0-is-here-offline-only-express-first-displayable-anywhere-fc444e5659ee). Although it's a relatively new data visualization package, we can pretty simply create interactive visualizations with only a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these examples, we'll work with the [Instacart](https://www.instacart.com/datasets/grocery-shopping-2017) data that we used to explore pivot tables and VLOOKUP in Excel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *order_products__prior.csv* dataset gives us a breakdown of the:  \n",
    " -  order ID number\n",
    " -  product ID numbers that were in that order\n",
    " -  the order the product was added to the customer's cart\n",
    " -  if the product was reordered (1). If the customer had not reordered or if this was their first order, this is coded as a 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order dataset\n",
    "# this gives us a breakdown of the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *products.csv* dataset gives us a breakdown of the:  \n",
    " -  product ID number\n",
    " -  product name\n",
    " -  product's aisle ID number\n",
    " -  product's department ID number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# products dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *aisles.csv* dataset gives us a breakdown of the:  \n",
    " - aisle ID number\n",
    " - aisle name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aisles dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *departments.csv* dataset gives us a breakdown of the: \n",
    " -  department ID number\n",
    " -  department name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# departments dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Merging Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand what this data tells us, we want to build some data visualizations to understand the trends and distributions of customers' orders and the most and least common products, department, and aisle categories to better market to and serve these customers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is pretty clean to begin with, but one thing that we want to do in order to know what products, aisles, and departments the ID numbers correspond with is merge their names to the *order_products_prior.csv* dataframe. We'll do this with [pandas merge function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html). There are several ways to merge and concatenate your data that are visually outlined [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html). We'll use the following code block to merge two dataframes together: \n",
    "\n",
    "```\n",
    "pd.merge(left_df, right_df, how='inner', on=None, left_on=None, right_on=None,\n",
    "         left_index=False, right_index=False, sort=True,\n",
    "         suffixes=('_x', '_y'), copy=True, indicator=False,\n",
    "         validate=None)\n",
    "```\n",
    "Where the __left_df__ and __right_df__ are the two dataframes that you want to merge together (it doesn't matter which one is the left and right as long as this remains consistent in the rest of your parameters.\n",
    "\n",
    "To perform a VLOOKUP in Excel, we type:\n",
    "`=VLOOKUP(lookup_value, table_array, col_index_number, [range_index])`\n",
    "The _lookup_value_ is essentially defining the *left_df* and the *table_array* is essentially defining the *right_df*. \n",
    "\n",
    "__how__ describes _how_ you're merging the dataframes together:\n",
    " - \"inner\" means that you're only merging the dataframes together on the values that are the same in __both__ dataframes. This also removes all data that isn't present in both dataframes. If you don't identify a _how_, then \"inner\" is the default merge function.\n",
    " - \"outer\" means that you're merging the dataframes on the values that are in both dataframes __and__ keeping the values in both dataframes that are not the same. The values in both left_df and right_df that don't have a match in the other dataset well have a NaN (not a number) in the row(s) where there is no match.\n",
    " - \"right\" means that you're merging the values from the left_df onto the right_df if they match. If there are rows in the right_df that don't have a match in the left_df, then these rows will have NaN (not a number) in the row where there is no match.\n",
    " - \"left\" means that you're merging the values from the right_df onto the left_df if they match. If there are rows in the left_df that don't have a match in the right_df, then these rows will have NaN (not a number) in the row where there is no match.\n",
    "\n",
    "When we used VLOOKUP in Excel, we would essentially use a \"right\" or \"left\" merge depending on which Excel spreadsheet we use for our *lookup_value* and *table_array*.\n",
    "\n",
    "__on, right_on, and left_on__ describe the columns on which you are merging the two columns:\n",
    " - use on = \"column name\" if both dataframes have the same column name to merge the values\n",
    " - use right_on = \"right_df column name\" and left_on = \"left_df column name\" if the right and left column names are different "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to merge the product, department, and aisle names onto the *order_products__prior* dataset where we use __df_orders__ as __right_df__ and __df_products, df_departments, and df_aisles__ as the __left_df__: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right now, our order dataset looks like this: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and our products dataset looks like this: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to merge all of the data from df_products onto df_orders so that we can later merge the aisle and department names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so now our orders dataframe looks like this: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only wanted to add on one or more specific columns, we would identify these when we define the __right_df__ and __left_df__. For example, if we only wanted to add on the product name and the aisle name columns, we'd type the data that we merge on + the columns that we want to merge in two square brackets (e.g. right_df[[\"match_column\", \"column1_tomerge\", \"column2_tomerge\"]]: \n",
    "```\n",
    "df_orders = pd.merge(df_orders, df_products[[\"product_id\", \"aisle_id\", \"department_id\"]], how = \"left\", on = \"product_id\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the department names onto our orders dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the aisle names onto our orders dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now our dataframe looks like: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big-Picture Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all of the product, department, and aisle names merged into one dataset. Before we aggregate and manipulate our data and create some data visualizations, we probably want to get some big-picture findings about our data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the dataframe look like?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique orders: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manipulate and show our data in a useful way, it might be useful to get the number of unique items in all of the columns. While we can do this by typing the above line for each column name, this can get tedious--especially if we had to do this for more than 9 columns!). In some cases, when we want to perform the same funtion to different parts of our dataframe, we can implement a __Python for loop__, which essentially \"loops\" through a series of commands to every item in a given list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how we can create a list of all of the column names in our dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we'll use a for loop to print out the number of unique items in each column \n",
    "\n",
    "# for each column name in our list of column names (which we printed above)\n",
    "    # create a variable that \"stores\" the number of unique items in that column (similar to what we did above)\n",
    "    # print out the column name + : + the number of unique items in this column\n",
    "    \n",
    "\n",
    "# the for loop will keep repeating this function (everything indented) until there are no more items in the column name list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to first get an idea of the top 20 most ordered items for each of the product, department, and aisle categories. We can do this in two ways: \n",
    "\n",
    "1. Creating an aggregated table with the counts of the categories, and then sorting and filtering the count column\n",
    "2. Adding an extra column with the count of that category and then sorting and filtering the count column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both instances we'll utilize the __groupby__ function. This allows us to *group* our columns into different categories and then perform functions on them--similar to how we could group column or row data in Excel pivot tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregating Column Counts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To aggregate our product column data by the number of items in each column, we'll use the following convention: \n",
    "\n",
    "`new_dataframe_name = old_data_frame_name.groupby(\"column_we_want_to_group_values_by\")[\"column_we_want_to_perform_group_calculation_on\"].agg([\"function_we_want_to_perform\"]).reset_index()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we'll aggregate the products column to see how many of each product were ordered in this data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preview our new dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort our dataframe to have the count column in descending order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the top 20 products ordered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Product Counts as an Extra Column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it might be more helpful to create a new column on our dataframe that lists the count, sum, or other calculation of another column in our dataset. For this we'll still use the __groupby__ function, but instead of __agg__-ing the values to groupby, we'll __transform__ them in a new column using the following convention: \n",
    "\n",
    "`dataframe_name[\"new_column_name\"] = data_frame_name.groupby(\"column_we_want_to_group_values_by\")[\"column_we_want_to_perform_group_calculation_on\"].transform(\"count\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column \"product_count\" in df_orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we haven't filtered our data for to only keep unique valuews, we'll also want to drop duplicated values with the same product name in order to see the top 20 ordered items using the following convention: \n",
    "\n",
    "`df = df.drop_duplicates(subset = \"column to drop duplicates for\", keep = \"first\", inplace = False)`\n",
    "\n",
    "Where the \n",
    " - __subset__ is the name of the column (or list of columns in square brackets) that we want to drop duplicates on\n",
    " - __keep__ is to identify whether we want to keep the \"first,\" \"last,\" or none (False) of the duplicates; default is _\"first\"_\n",
    " - __inplace__ is to identify whether we want to replace the dataframe with this \"new\" dropped duplicate data (True) or keep this manipulated data as a copy (False); default is _False_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe that drops all but the first value of each duplicate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort our data by the product_count column similar to above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the top 20 ordered products\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating dataframes can be useful if we don't need the extra data in our dataframes, but transforming data into a new column can be useful if we want to use the counts, sum, etc. as part of a larger analysis. Keep in mind that as our dataframe grows, it will become slower and more difficult to manage all of the data in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualizations with Plotly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a good idea of what our data tells us, we can make some interactive visualizations with Plotly to further explore and undertand what this data says. While there are many ways to use the Plotly package, the easiest way is to use this library is with their __[Plotly Express](https://plot.ly/python/plotly-express/)__ function. \n",
    "\n",
    "We'll review how to make bar, bubble, and pie charts with Plotly express, however, there are several more kinds of visualizations that you can make with plotly depending on how you want to display your data as shown in their newest [Medium](https://medium.com/plotly/plotly-py-4-0-is-here-offline-only-express-first-displayable-anywhere-fc444e5659ee) post and [Python tutorial gallery](https://plot.ly/python/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Charts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a simple bar chart of the counts of the products ordered in this dataset we'll follow the convention: \n",
    "\n",
    "```\n",
    "chart_name = px.bar(df, # dataframe of the data we want to plot\n",
    "                     x = \"column on the x axis\", \n",
    "                     y = \"column on the y axis\", \n",
    "                     color = \"how to categorize data with different colors\", \n",
    "                     hover_name = \"values we want to show up when we hover over the chart\", \n",
    "                    title = \"chart title\", \n",
    "                    labels = {\"column_name\": \"new label\", \"column_name\": \"new label\"}, # renaming labels\n",
    "                    #orientation = \"h\" # change the x, y values to make a horizontal bar chart\n",
    "                    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphing the entire dataset will take a really long time (and won't really be effective with 49677 different products)\n",
    "\n",
    "# we'll create a \"new\" dataframe of only the top 20 purchased foods: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphing the product counts in df_product_top20: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder the bar chart so that all of our data is all in descending order \n",
    "# instead of descending order by category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can [stylize](https://plot.ly/python/axes/) your data visualizations in several ways including colors, fonts, sizes, spacing, text rotation, tick marks, etc.\n",
    "\n",
    "Here, we'll change the font for the entire graph and the color of the tick mark labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the font of the text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we can easily export an image of our chart that we've created as a png file with the icons at the top of our visualization, sometimes it's useful to share interactive versions of the charts. While you may sometimes be able to share the code or notebook that you used to produce the code, you'll also need to make sure that the people running the code have the right packages, versions, and software to run the code, and that they know how to work with these documents. An easy way to share the visualizations without needing code is to export the grah as an HTML file with the following convention: \n",
    "\n",
    "```\n",
    "figure_name.write_html('title_for_html_doc.html', auto_open = True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export plotly chart as an HTML file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
